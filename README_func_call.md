# ä½¿ç”¨ChatGLM4è¿›è¡Œå¤§æ¨¡å‹å·¥å…·å¾®è°ƒï¼ˆé™„ä»£ç å’Œæµ‹è¯•è„šæœ¬ï¼‰

ä½œè€…ï¼šæƒ…æ„Ÿæœºå™¨å®éªŒå®¤-é™ˆå°‘å® é‚®ç®±ï¼šshaohon_chen@115lab.club

## æ‘˜è¦

æœ¬æ•™ç¨‹ä¸»è¦å®ç°äº†ä¸€ä¸ªå¤§æ¨¡å‹çš„å·¥å…·å¾®è°ƒæ–¹æ³•ã€‚ä¸ºäº†ä¾¿äºå®ç°ï¼Œå‡å°‘ä»£ç é‡ï¼Œæœ¬æ–‡ä½¿ç”¨äº†ğŸ¤—HuggingFaceçš„TRLæ¡†æ¶å®ç°ã€‚è¯¥æ¡†æ¶é™¤äº†æ”¯æŒSFTå¤–ï¼Œå¯¹DPOã€PPOã€GRPOç­‰æµè¡Œçš„å¼ºåŒ–å¾®è°ƒç®—æ³•éƒ½æœ‰å¾ˆå¥½çš„æ”¯æŒã€‚

è™½ç„¶ä½¿ç”¨æ¡†æ¶èƒ½å¤Ÿæå¤§çš„å‡å°‘å·¥ä½œé‡ï¼Œä½†æ˜¯ä¸å¯é¿å…çš„ä¸ºæ–°æ‰‹å­¦ä¹ å¸¦æ¥äº†å›°æ‰°ã€‚å› æ­¤æœ¬æ•™ç¨‹ä¼šå°½é‡é™„ä¸Šå®Œæ•´çš„æ–‡æ¡£å¼•ç”¨æ¥å¸®åŠ©è¯»è€…è¿›ä¸€æ­¥å­¦ä¹ æ¡†æ¶ã€‚è¯šç„¶ä»ä½¿ç”¨pytorchå®ç°å¾®è°ƒè¿‡ç¨‹èƒ½å¤Ÿæå¤§çš„æå‡å¯¹è¿‡ç¨‹çš„ç†è§£ï¼Œç¤¾åŒºä¹Ÿæœ‰ç›¸å½“å¤šä¼˜ç§€çš„é¡¹ç›®ã€‚ä½†æ˜¯ç¬”è€…ä»æ¨èå¤§å®¶å¤šä½¿ç”¨æ¡†æ¶æ¥å®Œæˆè®­ç»ƒï¼Œè¿™æ ·å¯ä»¥å‡å°‘å¤§é‡çš„æ—¶é—´æ¥è®©å¤§å®¶æ›´ä¸“æ³¨äºåˆ›æ–°ã€‚

å› æ­¤æœ¬æ•™ç¨‹å»ºè®®å¯¹ğŸ¤—HuggingFace Transformersæ¡†æ¶æœ‰ä¸€å®šåŸºç¡€çš„è¯»è€…é˜…è¯»ï½ã€‚

æ³¨æ„ï¼šç”±äºChatGLMçš„æ¨¡å‹ç›¸å¯¹è¾ƒå¤§ï¼Œå®é™…è¿è¡Œå¤§æ¦‚éœ€è¦æ˜¾å­˜>=16G

## ç›®å½•

**ç›®å½•ï¼š**



**å‚è€ƒèµ„æ–™ï¼š**

* æ™ºè°±AIå®˜ç½‘ï¼š[https://www.zhipuai.cn/](https://www.zhipuai.cn/)

* ChatGLM-9BåŸºåº§æ¨¡å‹ï¼š[https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)

* ChatGLM-9B-Chatæ¨¡å‹ï¼š[https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)

* glaiveå‡½æ•°è°ƒç”¨æ•°æ®é›†ä¸­æ–‡ç‰ˆï¼š[https://huggingface.co/datasets/llamafactory/glaive_toolcall_zh](https://huggingface.co/datasets/llamafactory/glaive_toolcall_zh)

* æœ¬åšå®¢å¼€æºé¡¹ç›®é“¾æ¥ï¼š[https://github.com/ShaohonChen/chatglm-finetune](https://github.com/ShaohonChen/chatglm-finetune)

* SwanLabè®­ç»ƒæ—¥å¿—æŸ¥çœ‹ï¼š[https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)

## TRLåŒ…ä»‹ç»+ç¯å¢ƒå‡†å¤‡

![./docs/trl](./docs/trl.png)

æœ¬æ•™ç¨‹ä½¿ç”¨[ğŸ¤—HuggingFace TRL](https://huggingface.co/docs/trl/index)æ¡†æ¶æ¥å®Œæˆå¾®è°ƒä»£ç çš„å®ç°ã€‚TRLæ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”ä¾¿äºä½¿ç”¨çš„å¾®è°ƒæ¡†æ¶ï¼Œé™¤äº†æ”¯æŒSFTå¤–ï¼Œä¹Ÿèƒ½è½»æ¾çš„é€šè¿‡æ¥å£è°ƒç”¨DPOã€PPOã€GRPOç­‰æµè¡Œçš„å¼ºåŒ–å¾®è°ƒç®—æ³•ã€‚æ­¤å¤–ä¹Ÿå®Œç¾å…¼å®¹Transformersæ¶æ„ã€‚

é¦–å…ˆæ˜¯å®‰è£…æœ¬æ•™ç¨‹çš„ç¯å¢ƒï¼Œå®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
pip install transformers trl datasets peft swanlab
```

å…¶ä¸­`transformers trl peft`ç”¨äºæ¨¡å‹çš„åŠ è½½å’Œè®­ç»ƒï¼Œ`datasets`ç”¨äºå¯¼å…¥æ•°æ®é›†ï¼Œ`swanlab`ç”¨äºå¯¹è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–è·Ÿè¸ªã€‚

ä¸‹é¢åˆ—ä¸¾ä¸€ä¸ªç®€å•çš„å¾®è°ƒæ¡ˆä¾‹æ¥ä»‹ç»HF TRLæ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•ï¼š

```python
from datasets import load_dataset
from trl import SFTConfig, SFTTrainer

dataset = load_dataset("stanfordnlp/imdb", split="train")   # è®¾ç½®å¾®è°ƒæ•°æ®é›†ï¼Œæ­¤å¤„ä½¿ç”¨IMDBç”µå½±è¯„è®ºåˆ†ç±»æ•°æ®

training_args = SFTConfig(  # è®¾ç½®å¾®è°ƒå‚æ•°
    max_length=512,
    output_dir="/tmp",
)
trainer = SFTTrainer(   # è®¾ç½®æ¨¡å‹ï¼Œæ­¤å¤„ä½¿ç”¨facebookçš„opt-350Mï¼Œå‚æ•°é‡æ¯”è¾ƒå°ä¾¿äºä¸‹è½½
    "facebook/opt-350m",
    train_dataset=dataset,
    args=training_args,
)
trainer.train() # å¼€å§‹è®­ç»ƒï¼Œæµç¨‹å’ŒTRLä¸€æ ·
```

ä¸Šé¢çš„ä»£ç æ¥è‡ªHFå®˜æ–¹æ–‡æ¡£[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)ï¼Œå¢åŠ äº†æ³¨é‡Šä¾¿äºè¯»è€…ç†è§£ã€‚

ç®€å•æ¥è¯´TRLåŒ…çš„ä½¿ç”¨æ–¹æ³•å’ŒTransformersç±»ä¼¼ï¼Œä¸è¿‡å¤šäº†ä¸¤æ­¥ï¼š

* å¯¼å…¥`SFTConfig`æ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—åŸºäº`transformers`çš„`TrainingArguments`ï¼Œä¸è¿‡é’ˆå¯¹SFTå¼•å…¥äº†ä¸€ç‚¹é¢å¤–çš„å‚æ•°ï¼Œä»¥åŠloraçš„æ”¯æŒå‚æ•°

* å¯¼å…¥`SFTTrainer`æ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—åŒ…å«äº†SFTçš„ä»£ç å®ç°ï¼Œè¿˜æœ‰ä¸€äº›å¯¹`peft`çš„loraæ”¯æŒå’Œæ•°æ®é›†æ ¼å¼è½¬æ¢ä»£ç ã€‚

åæ–‡å°†å®Œæ•´çš„ä»‹ç»å¦‚ä½•ä½¿ç”¨TRLåŒ…å®Œæˆå¤§æ¨¡å‹çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚

## ChatGLM4ä»‹ç»+æ¨¡å‹å‡†å¤‡

![chatglm_history](./docs/chatglm_history.png)

GLM-4-9Bæ˜¯[æ™ºè°±AI](https://www.zhipuai.cn/)æ¨å‡ºçš„æœ€æ–°ä¸€ä»£é¢„è®­ç»ƒæ¨¡å‹GLM-4ç³»åˆ—ä¸­çš„å¼€æºç‰ˆæœ¬ã€‚ChatGLMå‘å¸ƒäº†å¤šä¸ªç‰ˆæœ¬ï¼Œå…¶ä¸­GLM-4-9Bæ˜¯ç¬¬å››ä»£åŸºåº§æ¨¡å‹ï¼Œå…¶å¾®è°ƒç‰ˆæœ¬GLM-4-9B-Chatå…·å¤‡ç½‘é¡µæµè§ˆã€ä»£ç æ‰§è¡Œã€è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ï¼ˆFunction Callï¼‰å’Œé•¿æ–‡æœ¬æ¨ç†ï¼ˆæ”¯æŒæœ€å¤§ 128K ä¸Šä¸‹æ–‡ï¼‰ç­‰é«˜çº§åŠŸèƒ½ã€‚

æœ¬æ•™ç¨‹ä½¿ç”¨GLM-4-9Bæ¨¡å‹è¿›è¡Œå‡½æ•°è°ƒç”¨åŠŸèƒ½å¾®è°ƒï¼Œå¹¶ä½¿ç”¨SwanLabè¿›è¡Œæ¨¡å‹çš„ç»“æœè·Ÿè¸ªã€‚

âš ï¸æ³¨æ„ï¼šChatGLMä¸ºäº†é…åˆHuggingface Transformersæ›´æ–°ï¼Œå‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬æƒé‡`THUDM/glm-4-9b`å’Œ`THUDM/glm-4-9b-hf`ï¼Œåè€…å¯¹åº”æ›´ä¸ºæ–°ç‰ˆæœ¬çš„transformersï¼Œå› æ­¤æœ¬æ•™ç¨‹ä½¿ç”¨åè€…çš„æƒé‡ã€‚

æœ¬æ•™ç¨‹ä»¥ç»æä¾›å¥½äº†ä¸‹è½½æ¨¡å‹çš„è„šæœ¬ï¼Œä¸‹è½½æ¨¡å‹çš„æ–¹æ³•å¦‚ä¸‹ï¼š

```bash
huggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf
```

æ¨¡å‹å°†ä¼šä¸‹è½½åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„`./weights/glm-4-9b-hf`ä¸­

ä¸‹é¢åˆ—ä¸¾ä¸€ä¸ªä½¿ç”¨`transformers`åŠ è½½ChatGLMæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†çš„ä»£ç ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained("THUDM/glm-4-9b-chat-hf")
model = AutoModelForCausalLM.from_pretrained("THUDM/glm-4-9b-chat-hf").eval().to(device)
inputs = tokenizer.encode("æˆ‘æ˜¯ChatGLMï¼Œæ˜¯", return_tensors="pt").to(device)
outputs = model.generate(inputs)
print(tokenizer.decode(outputs[0]))
```

ç”±äºæ˜¯åŸºåº§æ¨¡å‹ï¼Œæ²¡ç»è¿‡å¾®è°ƒï¼Œå› æ­¤æ¨¡å‹åªä¼šå®Œæˆ`"æˆ‘æ˜¯ChatGLMï¼Œæ˜¯"`è¿™æ®µæ–‡æœ¬çš„åç»­è¡¥å…¨ï¼Œè¿è¡Œåä¼šç”Ÿæˆå¦‚ä¸‹ä»£ç ï¼š

```bash
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.35it/s]
[gMASK]<sop>æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘æ˜¯ChatGLMï¼Œæ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹
```

å½“ç„¶ä¸Šé¢çš„ä¾‹å­æ˜¯ä¸€ä¸ªåŸºåº§æ¨¡å‹æ¨ç†çš„ä¾‹å­ï¼Œè¯¥æ¨¡å‹åªèƒ½è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¦‚æœå¸Œæœ›ä½¿ç”¨å¯¹è¯èƒ½åŠ›ï¼Œè¿˜æ˜¯éœ€è¦åŠ è½½å·²ç»å¾®è°ƒå¥½çš„å¯¹è¯æ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from transformers import pipeline

messages = [
    {"role": "user", "content": "ä½ æ˜¯è°"},
]
pipe = pipeline("text-generation", model="THUDM/glm-4-9b-chat-hf")
print(pipe(messages))
```

æ­¤å¤„æˆ‘ä»¬æ¢äº†ç§æ¨ç†æ¥å£ï¼Œç›´æ¥ä½¿ç”¨pipelineå®Œæˆæ¨ç†ï¼Œè¿è¡Œåå°†ä¼šç”Ÿæˆå¦‚ä¸‹ä¿¡æ¯

```bash
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.24it/s]
Device set to use cuda:0
[{'generated_text': [{'role': 'user', 'content': 'ä½ æ˜¯è°'}, {'role': 'assistant', 'content': '\næˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåä¸º ChatGLMã€‚æˆ‘æ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œ'}]}]
```

ä½¿ç”¨`print(model)`å°†æ¨¡å‹çš„ç»“æ„æ‰“å°å‡ºæ¥ï¼Œå±•ç¤ºå¦‚ä¸‹ï¼š

```text
GlmForCausalLM(
  (model): GlmModel(
    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)
    (layers): ModuleList(
      (0-39): 40 x GlmDecoderLayer(
        (self_attn): GlmAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
          (k_proj): Linear(in_features=4096, out_features=256, bias=True)
          (v_proj): Linear(in_features=4096, out_features=256, bias=True)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): GlmMLP(
          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)
          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)
          (activation_fn): SiLU()
        )
        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)
        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)
      )
    )
    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)
    (rotary_emb): GlmRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)
)
```

å¯ä»¥çœ‹åˆ°GLMæ¨¡å‹çš„å±‚æ•°è¾¾åˆ°äº†æƒŠäººçš„40å±‚ğŸ˜‚ï¼Œå› æ­¤æœ¬èº«ä½¿ç”¨Loraè¿›è¡Œå¾®è°ƒæ—¶å…¶å¯è®­ç»ƒå‚æ•°ä¼šæ¯”å…¶ä»–æ¨¡å‹å¤§ä¸€äº›ã€‚

## æ•°æ®é›†å‡†å¤‡

æ•°æ®é›†æˆ‘å·²ç»æå‰åŒ…æ‹¬åœ¨äº†githubé¡¹ç›®å½“ä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ä¸‹è½½å®Œæ•´çš„å®éªŒä»£ç 

```bash
git clone https://github.com/ShaohonChen/chatglm-finetune.git
```

å¦‚æœåªæƒ³ä¸‹è½½æ•°æ®é›†ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½å¦‚ä¸‹æ–‡ä»¶ï¼š

```bash
...
```

## ä»£ç è¯´æ˜+è¶…å‚æ•°è°ƒæ•´

å®Œæ•´çš„å¾®è°ƒä»£ç å…¬å¼€åœ¨äº†GitHubä¸Šï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å³å¯ä¸‹è½½

```bash
git clone https://github.com/ShaohonChen/chatglm-finetune.git
```

æ–‡ç« çš„é™„ä»¶ä¸­ä¹Ÿæœ‰å®Œæ•´çš„å®ç°ä»£ç [#ä»£ç é™„ä»¶](#é™„ä»¶å®Œæ•´ä»£ç )

æœ¬æ–‡æ¥ä¸‹æ¥é‡ç‚¹ä»‹ç»å„ä¸ªä»£ç çš„åŠŸèƒ½æ¨¡å—

åŠ è½½æ¨¡å‹çš„è¶…å‚æ•°è®¾ç½®ï¼Œè¿™é‡Œå¯ä»¥é‡ç‚¹å…³æ³¨loraå‚æ•°çš„è®¾ç½®ï¼Œæœ¬æ–‡loraå‚æ•°å‚è€ƒäº†ChatGLMå®˜æ–¹å¾®è°ƒä»£ç çš„loraå‚æ•°è®¾ç½®

```python
################
# Model kwargs
################
@dataclass
class ChatGLM4ModelConfig(ModelConfig):
    model_name_or_path: Optional[str] = field(
        default="./weights/glm-4-9b-hf",
        # default="/data/nvme1/weights/Qwen2.5-7B",
        metadata={
            "help": "Model checkpoint for weights initialization. default used glm4"
        },
    )
    torch_dtype: Optional[str] = field(
        default="bfloat16",
        metadata={
            "help": "Override the default `torch.dtype` and load the model under this dtype.",
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    use_peft: bool = field(
        default=True,
        metadata={"help": "Whether to use PEFT for training. Default true"},
    )
    lora_r: int = field(
        default=8,
        metadata={"help": "LoRA R value."},
    )
    lora_alpha: int = field(
        default=32,
        metadata={"help": "LoRA alpha."},
    )
    lora_dropout: float = field(
        default=0.1,
        metadata={"help": "LoRA dropout."},
    )
    lora_target_modules: Optional[list[str]] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj"],
        metadata={"help": "LoRA target modules."},
    )
```

æ•°æ®é›†è¶…å‚æ•°è®¾ç½®ï¼Œè¿™é‡Œæ¯”è¾ƒç®€å•ï¼Œåªæ˜¯åŠ è½½äº†æœ¬åœ°çš„glaiveæ•°æ®é›†

```python
################
# Datasets kwargs
################
@dataclass
class DataTrainingArguments:
    data_files: Optional[str] = field(
        default="./data/glaive_toolcall_zh_1k.json",
        metadata={"help": "The name of the dataset to use (via the datasets library)."},
    )

```

ä¸è¿‡ä¸ºäº†æ–¹ä¾¿è¯»è€…ç†è§£æ•°æ®é›†é•¿ä»€ä¹ˆæ ·ï¼Œä»æ—§æä¾›æ•°æ®é›†å±•ç¤ºè„šæœ¬

```python
import datasets
raw_dataset=datasets.load_dataset("json", data_files="data/glaive_toolcall_zh_1k.json")
print(raw_dataset)
"""æ‰“å°å†…å®¹
DatasetDict({
    train: Dataset({
        features: ['conversations', 'tools'],
        num_rows: 1000
    })
})
"""
```

å¯ä»¥çœ‹åˆ°æ•°æ®ä¸€å…±æœ‰1000æ¡ï¼Œå¹¶ä¸”åŒ…æ‹¬`'conversations', 'tools'`ä¸¤ä¸ªå­—æ®µ

è¿›ä¸€æ­¥é€‰å–å…¶ä¸­ä¸€æ¡æ‰“å°ï¼š

```python
print(raw_dataset["train"][0])
```

è¾“å‡ºå¦‚ä¸‹ï¼š

```json
{'conversations': [{'from': 'human', 'value': 'ä½ å¥½ï¼Œæˆ‘éœ€è¦ä¸€ä¸ª1åˆ°100ä¹‹é—´çš„éšæœºæ•°ã€‚'},
  {'from': 'function_call',
   'value': '{"name": "generate_random_number", "arguments": {"min": 1, "max": 100}}'},
  {'from': 'observation', 'value': '{"number": 57}'},
  {'from': 'gpt', 'value': 'ç”Ÿæˆçš„éšæœºæ•°åœ¨1åˆ°100ä¹‹é—´ï¼Œæ˜¯57ã€‚'},
  {'from': 'human', 'value': 'å¥½çš„ï¼Œå¯ä»¥ã€‚è¿™æ¬¡ç”Ÿæˆä¸€ä¸ªé•¿åº¦åœ¨200åˆ°300ä¹‹é—´çš„å¥å­ã€‚'},
  {'from': 'function_call',
   'value': '{"name": "generate_random_number", "arguments": {"min": 200, "max": 300}}'},
  {'from': 'observation', 'value': '{"number": 267}'},
  {'from': 'gpt', 'value': 'ç”Ÿæˆçš„éšæœºæ•°åœ¨200åˆ°300ä¹‹é—´ï¼Œæ˜¯267ã€‚'},
  {'from': 'human', 'value': 'è°¢è°¢ï¼Œè¿™äº›å°±æ˜¯æˆ‘éœ€è¦çš„å…¨éƒ¨ã€‚'},
  {'from': 'gpt', 'value': 'ä¸å®¢æ°”ï¼å¦‚æœä½ è¿˜éœ€è¦å…¶ä»–ä»€ä¹ˆï¼Œéšæ—¶é—®ã€‚'}],
 'tools': '[{"name": "generate_random_number", "description": "åœ¨æŒ‡å®šèŒƒå›´å†…ç”Ÿæˆä¸€ä¸ªéšæœºæ•°", "parameters": {"type": "object", "properties": {"min": {"type": "integer", "description": "æœ€å°å€¼"}, "max": {"type": "integer", "description": "æœ€å¤§å€¼"}}, "required": ["min", "max"]}}]'}
```

å¯ä»¥çœ‹å‡ºæ•°æ®é›†çš„`conversations`éƒ¨åˆ†å’Œ`tools`éƒ¨åˆ†åˆ†åˆ«å®šä¹‰äº†æ¨¡å‹çš„é—®ç­”è¿‡ç¨‹ï¼Œå’Œèƒ½å¤Ÿè°ƒç”¨çš„å‡½æ•°ã€‚è¿™é‡Œæ³¨æ„ï¼Œ`tools`éƒ¨åˆ†å¹¶ä¸æ€»æ˜¯æœ‰èƒ½è°ƒç”¨çš„å‡½æ•°ï¼Œå¯èƒ½å‡ºç°ä¸ºç©ºçš„æƒ…å†µã€‚

ChatGLMæä¾›çš„æ¨èè¾“å…¥toolsæ•°æ®ç»“æ„å¦‚ä¸‹ï¼š

```json
{
  "messages": [
    {
      "role": "system",
      "content": "",
      "tools": [
        {
          "type": "function",
          "function": {
            "name": "get_recommended_books",
            "description": "Get recommended books based on user's interests",
            "parameters": {
              "type": "object",
              "properties": {
                "interests": {
                  "type": "array",
                  "items": {
                    "type": "string"
                  },
                  "description": "The interests to recommend books for"
                }
              },
              "required": [
                "interests"
              ]
            }
          }
        }
      ]
    },
    {
      "role": "user",
      "content": "Hi, I am looking for some book recommendations. I am interested in history and science fiction."
    },
    {
      "role": "assistant",
      "content": "{\"name\": \"get_recommended_books\", \"arguments\": {\"interests\": [\"history\", \"science fiction\"]}}"
    },
    {
      "role": "observation",
      "content": "{\"books\": [\"Sapiens: A Brief History of Humankind by Yuval Noah Harari\", \"A Brief History of Time by Stephen Hawking\", \"Dune by Frank Herbert\", \"The Martian by Andy Weir\"]}"
    },
    {
      "role": "assistant",
      "content": "Based on your interests in history and science fiction, I would recommend the following books: \"Sapiens: A Brief History of Humankind\" by Yuval Noah Harari, \"A Brief History of Time\" by Stephen Hawking, \"Dune\" by Frank Herbert, and \"The Martian\" by Andy Weir."
    }
  ]
}
```

è¿™é‡Œå¯èƒ½æœ‰ä¸€å®šç»éªŒçš„è¯»è€…ä¼šè¯´ï¼Œä¸å¯¹å‘€ï¼Œæˆ‘ä»¬ä»0è®­ç»ƒæˆ‘ä»¬å½“ç„¶å¯ä»¥å®šä¹‰è‡ªå·±çš„æ•°æ®ç»“æ„ã€‚è¿™ä¹ˆæƒ³æ˜¯å¯¹çš„ï¼Œä½†æ˜¯è®©æˆ‘ä»¬èƒ½å¤Ÿç›´æ¥ä½¿ç”¨ChatGLMåŸç”Ÿçš„`chat_template`ï¼Œæˆ‘è¿˜æ˜¯å»ºè®®å’±ä»¬éµå®ˆchatglmå®˜æ–¹å®šä¹‰çš„æ•°æ®æ ¼å¼ï¼Œè¿™ä¹ˆåšçš„è¯æ—¢èƒ½å…¼å®¹ChatGLMçš„å¾ˆå¤šå·¥å…·ï¼Œåˆèƒ½å……åˆ†åˆ©ç”¨å®˜æ–¹å®šä¹‰çš„special_tokenã€‚

è¿™é‡Œæœ‰ä¸€ä¸ªå°å‘æ˜¯å®˜æ–¹GitHubä¸­å¯¹äºToolså·¥å…·çš„æ•°æ®ç»“æ„æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› æ­¤è¿˜æ˜¯éœ€è¦å°†æ•°æ®é›†æ¸…æ´—ä¸ºå¦‚ä¸Šçš„æ•°æ®ç»“æ„ï¼æœ¬å¾®è°ƒæ•™ç¨‹çš„æ•°æ®é›†è½¬æ¢å‡½æ•°å¦‚ä¸‹ï¼š

```python
def formatting_func(example):
    """
    process data format
    """
    tools = []
    try:
        tool_list = json.loads(example["tools"])
    except:
        tool_list = []
    for tool in tool_list:
        tools.append(
            {
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": tool["parameters"],
                },
            },
        )
    conversations = [{"role": "system", "content": "", "tools": tools}]
    for chat in example["conversations"]:
        if chat["from"] == "human":
            role = "users"
        elif chat["from"] == "observation":
            role = "observation"
        else:
            role = "assistant"
        conversations.append({"role": role, "content": chat["value"]})
    return tokenizer.apply_chat_template(conversation=conversations, tokenize=False)
```

è¿™é‡Œæˆ‘ä»¬ç®€å•æ‰“å°ä¸€ä¸‹è½¬æ¢å®Œæˆåæ•°æ®é›†æœ€ç»ˆçš„ä¸€ä¸ªæ•ˆæœï¼Œå‚è€ƒè„šæœ¬å¦‚ä¸‹ï¼š

```python
import json
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("weights/glm-4-9b-chat-hf")


def formatting_func(example):
    """
    process data format
    """
    tools = []
    try:
        tool_list = json.loads(example["tools"])
    except:
        tool_list = []
    for tool in tool_list:
        tools.append(
            {
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": tool["parameters"],
                },
            },
        )
    conversations = [{"role": "system", "content": "", "tools": tools}]
    for chat in example["conversations"]:
        if chat["from"] == "human":
            role = "users"
        elif chat["from"] == "observation":
            role = "observation"
        else:
            role = "assistant"
        conversations.append({"role": role, "content": chat["value"]})
    return tokenizer.apply_chat_template(conversation=conversations, tokenize=False)

print(formatting_func(raw_dataset["train"][0]))
```

è¾“å‡ºæ•ˆæœå¦‚ä¸‹ï¼Œä»¥ä¸‹å­—æ®µä¾¿æ˜¯å®é™…è¿ç”¨äºæ¨¡å‹å¾®è°ƒæ—¶ï¼Œè¾“å…¥ç»™æ¨¡å‹çš„æ•°æ®æ ·å¼ï¼š

```text
[gMASK]<sop><|system|>
ä½ æ˜¯ä¸€ä¸ªåä¸º ChatGLM çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ æ˜¯åŸºäºæ™ºè°±AIè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ GLM-4 æ¨¡å‹å¼€å‘çš„ï¼Œä½ çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚

# å¯ç”¨å·¥å…·
## generate_random_number

{
    "name": "generate_random_number",
    "description": "åœ¨æŒ‡å®šèŒƒå›´å†…ç”Ÿæˆä¸€ä¸ªéšæœºæ•°",
    "parameters": {
        "type": "object",
        "properties": {
            "min": {
                "type": "integer",
                "description": "æœ€å°å€¼"
            },
            "max": {
                "type": "integer",
                "description": "æœ€å¤§å€¼"
            }
        },
        "required": [
            "min",
            "max"
        ]
    }
}
åœ¨è°ƒç”¨ä¸Šè¿°å‡½æ•°æ—¶ï¼Œè¯·ä½¿ç”¨ Json æ ¼å¼è¡¨ç¤ºè°ƒç”¨çš„å‚æ•°ã€‚<|users|>
ä½ å¥½ï¼Œæˆ‘éœ€è¦ä¸€ä¸ª1åˆ°100ä¹‹é—´çš„éšæœºæ•°ã€‚<|assistant|>
{"name": "generate_random_number", "arguments": {"min": 1, "max": 100}}<|observation|>
{"number": 57}<|assistant|>
ç”Ÿæˆçš„éšæœºæ•°åœ¨1åˆ°100ä¹‹é—´ï¼Œæ˜¯57ã€‚<|users|>
å¥½çš„ï¼Œå¯ä»¥ã€‚è¿™æ¬¡ç”Ÿæˆä¸€ä¸ªé•¿åº¦åœ¨200åˆ°300ä¹‹é—´çš„å¥å­ã€‚<|assistant|>
{"name": "generate_random_number", "arguments": {"min": 200, "max": 300}}<|observation|>
{"number": 267}<|assistant|>
ç”Ÿæˆçš„éšæœºæ•°åœ¨200åˆ°300ä¹‹é—´ï¼Œæ˜¯267ã€‚<|users|>
è°¢è°¢ï¼Œè¿™äº›å°±æ˜¯æˆ‘éœ€è¦çš„å…¨éƒ¨ã€‚<|assistant|>
ä¸å®¢æ°”ï¼å¦‚æœä½ è¿˜éœ€è¦å…¶ä»–ä»€ä¹ˆï¼Œéšæ—¶é—®ã€‚
```

æœ€åä¾¿æ˜¯è®­ç»ƒçš„è¶…å‚æ•°è®¾ç½®å’Œè®­ç»ƒè¿‡ç¨‹çš„å®ç°ï¼Œè¿™é‡Œç”±äºæ•°æ®è§„æ¨¡æ¯”è¾ƒå°ï¼Œæˆ‘ä»¬è®­ç»ƒ600ä¸ªstepsï¼Œæ¯ä¸ªGPUå®é™…batchå¤§å°ä¸º1*4ï¼š

```python
################
# Train kwargs
################
@dataclass
class MySFTConfig(SFTConfig):
    output_dir: Optional[str] = field(
        default="glm4-9b-toolcall",
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written. Defaults to 'glm4-9b-toolcall' if not provided."
        },
    )
    max_steps: int = field(
        default=600,
        metadata={
            "help": "If > 0: set total number of training steps to perform. Override num_train_epochs."
        },
    )
    per_device_train_batch_size: int = field(
        default=1,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for training."},
    )
    per_device_eval_batch_size: int = field(
        default=4,
        metadata={"help": "Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation."},
    )
    gradient_accumulation_steps: int = field(
        default=2,
        metadata={
            "help": "Number of updates steps to accumulate before performing a backward/update pass."
        },
    )
    learning_rate: float = field(
        default=5e-4, metadata={"help": "The initial learning rate for AdamW."}
    )
    bf16: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
                " architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change."
            )
        },
    )
    bf16_full_eval: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"
                " change."
            )
        },
    )
    max_seq_length: Optional[int] = field(
        default=512z,
        metadata={
            "help": "Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated "
            "from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the "
            "sequence length."
        },
    )
    eval_strategy: Union[str] = field(
        default="steps",
        metadata={"help": "The evaluation strategy to use."},
    )
    eval_steps: Optional[float] = field(
        default=0.2,
        metadata={
            "help": (
                "Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
    logging_steps: float = field(
        default=10,
        metadata={
            "help": (
                "Log every X updates steps. Should be an integer or a float in range `[0,1)`. "
                "If smaller than 1, will be interpreted as ratio of total training steps."
            )
        },
    )
```

è®­ç»ƒçš„æµç¨‹è¿™å—å¦‚ä¸‹,ä½¿ç”¨HF TRLåæµç¨‹å˜å¾—éå¸¸ç®€æ´ã€‚

```python
    ################
    # Training
    ################
    trainer = SFTTrainer(
        model=model_args.model_name_or_path,
        args=training_args,
        data_collator=None,
        train_dataset=raw_datasets["train"],
        eval_dataset=(
            raw_datasets["test"] if training_args.eval_strategy != "no" else None
        ),
        processing_class=tokenizer,
        peft_config=get_peft_config(model_args),
        formatting_func=formatting_func,
    )
    trainer.train()

    # Save
    trainer.save_model(training_args.output_dir)
```

## å¯åŠ¨è®­ç»ƒ+æ•ˆæœè¯„æµ‹

å¯åŠ¨è®­ç»ƒçš„å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
python train.py
```

å¯ä»¥çœ‹åˆ°å¦‚ä¸‹å¯åŠ¨ä¿¡æ¯

![train](docs/train.png)

å¦‚æœæ²¡ç™»å½•SwanLabå¯èƒ½ä¼šå¼¹å‡ºç™»å½•æç¤ºï¼Œè¿™é‡Œæ¨èé€‰æ‹©1å¹¶åœ¨[https://swanlab.cn](https://swanlab.cn)å®Œæˆæ³¨å†Œã€‚å³å¯åœ¨çº¿æŸ¥çœ‹åˆ°è®­ç»ƒè¿›å±•ã€‚

ç™»é™†å‘½ä»¤å¦‚ä¸‹

```bash
swanlab login
```

åœ¨çº¿è®­ç»ƒçœ‹æ¿å±•ç¤ºï¼š

[swanlab](docs/swanlab.png)

**å¤šå¡å®éªŒ**

å¦‚æœä½ çš„å¡æ•°æ¯”è¾ƒå¤šï¼Œæ¨èä½¿ç”¨å¤šå¡è®­ç»ƒæ¥æå¤§æå‡è®­ç»ƒé€Ÿåº¦ï¼é¦–å…ˆå®‰è£…huggingface accelerateå’Œdeepspeedæ¥æ–¹ä¾¿çš„å¼€å¯zero2å¤šå¡è®­ç»ƒï¼š

```bash
pip install accelerate deepspeed
```

æ¥ä¸‹æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤æ¥å¼€å¯å¤šå¡è®­ç»ƒï¼ˆé»˜è®¤8GPUï¼Œå¯æ›´æ”¹num_processeså‚æ•°ä¸ºå®é™…å¡æ•°ï¼‰ï¼š

```bash
accelerate launch --num_processes 8 --config_file configs/zero2.yaml train.py
```

å…³äºzero2çš„è¯¦ç»†è®¾ç½®åœ¨`configs/zero2.yaml`ä¸­ã€‚

**æ•ˆæœå¯¹æ¯”**

è¿™é‡Œæˆ‘ä»¬å¯¹æ¯”Qwen2.5-7Bæ¨¡å‹å’ŒGLM-4Bçš„å¾®è°ƒæ•ˆæœè¡¨ç°



## é™„ä»¶ï¼šå®Œæ•´ä»£ç 